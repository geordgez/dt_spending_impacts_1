{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "from os import path\n",
    "from collections import Counter\n",
    "import copy\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import division, unicode_literals\n",
    "from __future__ import print_function\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using http://peekaboo-vision.blogspot.com/2012/11/a-wordcloud-in-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ignore_words = ['i','years',']','laughter','plan','way','lot','thing','things','matter',\n",
    "                'reason',' ','[','%','day','year',\n",
    "                'trump','donald','hillary','clinton','romney','obama','mccain','barack',\n",
    "               'mitt','romney','kerry',\n",
    "               'today','families','country','time','state','states','something','right',\n",
    "               'kind',\n",
    "               'people','children','president','home','part',\n",
    "               'senator','everything','problem','future','issue','question','hampshire',\n",
    "               'election','bush','member','audience','everybody','folks','applause',\n",
    "               'cheers','thank','thanks',\n",
    "               'america', 'americans', 'family', 'campaign', 'administration',\n",
    "               'members', 'government', 'world', 'number']\n",
    "\n",
    "all_campaigns = [\"Kerry_2004\",\"Obama_2008\",\"McCain_2008\",\"Obama_2012\",\n",
    "                 \"Romney_2012\",\"Hillary_2016\",\"Trump_2016\"]\n",
    "\n",
    "for campaign_str in all_campaigns:\n",
    "    campaign_file = \"../out/\" + campaign_str + \".json\"\n",
    "\n",
    "    with open(campaign_file, \"r\") as infile:\n",
    "        campaign_data = json.load(infile)\n",
    "\n",
    "    ### Aggregated speeches\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    all_speeches = \"\"\n",
    "\n",
    "    for title in campaign_data:\n",
    "        #all_speeches += campaign_data[title]['speech']\n",
    "        all_speeches += stemmer.stem(campaign_data[title]['speech'])\n",
    "    \n",
    "    ### Using NLTK\n",
    "    \n",
    "    nltk_tokens = word_tokenize(all_speeches)\n",
    "    nouns = [token for token, pos in pos_tag(nltk_tokens) if pos.startswith('N')]\n",
    "    df_noun_count = pd.DataFrame(pd.Series(dict(Counter(nouns))), columns=[\"count\"])\n",
    "    df_noun_count['word'] = df_noun_count.index\n",
    "    df_noun_count = df_noun_count.reset_index(drop=True)\n",
    "    df_noun_count['word'].map(lambda x: (re.sub('[^A-Za-z]+', ' ',x)))\n",
    "    df_noun_count = df_noun_count[df_noun_count['word'].map(\n",
    "            lambda x: (x not in ignore_words) and len(x) > 1)]\n",
    "    \n",
    "    df_noun_count_small = df_noun_count.sort(\"count\", ascending=False).head(1000)\n",
    "    df_noun_count_small.head(50)\n",
    "    df_noun_count_small.to_csv('../out/' + campaign_str + '_top_words.csv')\n",
    "\n",
    "    ### Paste back into wordcloud with reduced file\n",
    "    df_compressed_count = copy.deepcopy(df_noun_count)\n",
    "    df_compressed_count['count'] = df_compressed_count['count'] / 10\n",
    "    df_compressed_count['count'] = df_compressed_count['count'].astype(int)\n",
    "    df_compressed_count = df_compressed_count[df_compressed_count['count'] > 1]\n",
    "\n",
    "    #df_compressed_count.sort(\"count\", ascending=False)\n",
    "    new_string = \"\"\n",
    "    for idx, row in df_compressed_count.iterrows():\n",
    "        new_string += (row[\"word\"] + \" \") * row[\"count\"] + \" \"\n",
    "    cv = CountVectorizer(min_df=0, stop_words=\"english\", max_features=200)\n",
    "    counts = cv.fit_transform([new_string]).toarray().ravel()\n",
    "    scaled_counts = counts / float(counts.max())\n",
    "    words = np.array(cv.get_feature_names())\n",
    "    \n",
    "    #generate word cloud\n",
    "    wordcloud = WordCloud(margin=20).generate(new_string)\n",
    "    plt.figure(1, figsize=(12,12))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.savefig('../img/' + campaign_str + '_word_cloud.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
